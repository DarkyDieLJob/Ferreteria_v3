# ğŸ—ï¸ Arquitectura del Sistema de Pruebas

## VisiÃ³n General

`core_testing` es un sistema completo de gestiÃ³n de pruebas para la aplicaciÃ³n de FerreterÃ­a que incluye:

- ğŸ–¥ï¸ **Dashboard Interactivo**: VisualizaciÃ³n en tiempo real del estado de las pruebas
- ğŸ“Š **Reportes de Cobertura**: AnÃ¡lisis detallado de la cobertura de cÃ³digo
- âš¡ **EjecuciÃ³n de Pruebas**: Herramientas para ejecutar pruebas de forma controlada
- ğŸ“ˆ **MÃ©tricas y AnÃ¡lisis**: Seguimiento de tendencias y rendimiento de pruebas

## Objetivos del Sistema

1. Proporcionar visibilidad en tiempo real del estado de las pruebas
2. Facilitar la identificaciÃ³n de problemas de calidad del cÃ³digo
3. Integrarse con el flujo de desarrollo existente
4. Ofrecer mÃ©tricas Ãºtiles para la toma de decisiones

## ğŸ—‚ï¸ Estructura del Proyecto (Actualizada: 06/07/2025)

```
core_testing/
â”œâ”€â”€ models.py               # Modelos de datos para pruebas y cobertura
â”œâ”€â”€ views/                  # Vistas del sistema de pruebas
â”‚   â”œâ”€â”€ __init__.py         # ExportaciÃ³n de vistas
â”‚   â”œâ”€â”€ views.py            # Vistas principales
â”‚   â””â”€â”€ views_test.py       # Pruebas de las vistas
â”‚
â”œâ”€â”€ templates/              # Plantillas del dashboard
â”‚   â”œâ”€â”€ core_testing/       # Plantillas especÃ­ficas del mÃ³dulo
â”‚   â”‚   â”œâ”€â”€ dashboard.html  # Panel principal
â”‚   â”‚   â”œâ”€â”€ testrun_list.html  # Lista de ejecuciones
â”‚   â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                  # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_basic.py       # Pruebas bÃ¡sicas
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ utils/                  # Utilidades
â”œâ”€â”€ management/             # Comandos personalizados
â””â”€â”€ testing_interfaces/     # Interfaces de prueba personalizadas
â”‚   â”œâ”€â”€ core_testing/
â”‚   â”‚   â”œâ”€â”€ base_testing.html    # Plantilla base para el Ã¡rea de testing
â”‚   â”‚   â”œâ”€â”€ dashboard.html       # Vista principal del dashboard
â”‚   â”‚   â”œâ”€â”€ coverage_report.html # Reporte detallado de cobertura
â”‚   â”‚   â”œâ”€â”€ testrun_detail.html  # Detalle de ejecuciÃ³n de pruebas
â”‚   â”‚   â””â”€â”€ testrun_list.html    # Listado histÃ³rico de ejecuciones
â”‚   â””â”€â”€ includes/                # Componentes reutilizables
â”‚       â”œâ”€â”€ _test_metrics.html   # Widget de mÃ©tricas
â”‚       â””â”€â”€ _coverage_chart.html # GrÃ¡ficos de cobertura
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ core_testing/
â”‚       â”œâ”€â”€ css/
â”‚       â”‚   â””â”€â”€ dashboard.css    # Estilos del dashboard
â”‚       â””â”€â”€ js/
â”‚           â””â”€â”€ dashboard.js     # LÃ³gica del frontend
â”‚
â”œâ”€â”€ management/
â”‚   â””â”€â”€ commands/
â”‚       â”œâ”€â”€ run_tests.py         # Comando para ejecutar pruebas
â”‚       â””â”€â”€ update_coverage.py   # ActualizaciÃ³n de mÃ©tricas
â”‚
â”œâ”€â”€ templatetags/           # Filtros y tags personalizados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ testing_filters.py  # Filtros para templates
â”‚
â”œâ”€â”€ tests/                  # Pruebas del mÃ³dulo
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_views.py
â”‚   â””â”€â”€ test_commands.py
â”‚
â”œâ”€â”€ views/                 # Vistas del dashboard
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ views.py
â”‚
â”œâ”€â”€ urls.py                # Rutas del mÃ³dulo
â”œâ”€â”€ admin.py              # ConfiguraciÃ³n de admin
â”œâ”€â”€ apps.py               # ConfiguraciÃ³n de la aplicaciÃ³n
â””â”€â”€ signals.py            # SeÃ±ales para eventos del sistema
```

## ğŸ§© Componentes Clave

### 1. Modelos de Datos

#### TestRun
- **PropÃ³sito**: Almacena informaciÃ³n sobre ejecuciones de pruebas
- **Campos Principales**:
  - `start_time`, `end_time`: Marca de tiempo de inicio/fin
  - `status`: Estado de la ejecuciÃ³n (en progreso, completado, fallido)
  - `total_tests`, `passed`, `failed`, `skipped`: EstadÃ­sticas
  - `coverage_percentage`: Cobertura de cÃ³digo
  - `duration`: DuraciÃ³n total de la ejecuciÃ³n
- **Relaciones**:
  - `test_cases`: RelaciÃ³n uno a muchos con TestCase
  - `module`: MÃ³dulo bajo prueba

#### TestCase
- **PropÃ³sito**: Detalles de casos de prueba individuales
- **Campos Principales**:
  - `name`: Nombre descriptivo del caso
  - `status`: Estado (passed, failed, skipped, error)
  - `duration`: Tiempo de ejecuciÃ³n
  - `error_message`: Mensaje de error (si aplica)
  - `stack_trace`: Traza de pila (en caso de fallo)
- **Relaciones**:
  - `test_run`: EjecuciÃ³n a la que pertenece
  - `test_suite`: Suite de pruebas relacionada

#### CoverageData
- **PropÃ³sito**: Almacena mÃ©tricas de cobertura de cÃ³digo
- **Campos Principales**:
  - `module`: MÃ³dulo analizado
  - `line_coverage`: Porcentaje de cobertura de lÃ­neas
  - `branch_coverage`: Porcentaje de cobertura de ramas
  - `timestamp`: Fecha de generaciÃ³n del reporte

### 2. Vistas y API

#### DashboardView
- **URL**: `/testing/dashboard/`
- **MÃ©todos**: GET
- **DescripciÃ³n**: Vista principal del dashboard con resumen de mÃ©tricas

#### TestRunDetailView
- **URL**: `/testing/runs/<int:pk>/`
- **MÃ©todos**: GET
- **DescripciÃ³n**: Detalle de una ejecuciÃ³n de pruebas especÃ­fica

#### CoverageReportView
- **URL**: `/testing/coverage/`
- **MÃ©todos**: GET
- **DescripciÃ³n**: Reporte detallado de cobertura de cÃ³digo

### 3. Comandos de GestiÃ³n

#### run_tests
```bash
python manage.py run_tests [opciones]
```
**Opciones**:
- `--module`: Ejecutar pruebas de un mÃ³dulo especÃ­fico
- `--coverage`: Generar reporte de cobertura
- `--parallel`: Ejecutar pruebas en paralelo
- `--keepdb`: Preservar la base de datos de pruebas

#### update_coverage
```bash
python manage.py update_coverage
```
**PropÃ³sito**: Actualiza las mÃ©tricas de cobertura sin ejecutar pruebas

### 4. Sistema de Eventos

El mÃ³dulo utiliza seÃ±ales de Django para reaccionar a eventos importantes:

1. **post_test_run**: DespuÃ©s de ejecutar pruebas
   - Actualiza mÃ©tricas
   - Genera notificaciones
   - Actualiza el dashboard

2. **coverage_updated**: Cuando se actualiza la cobertura
   - Almacena mÃ©tricas histÃ³ricas
   - Actualiza las estadÃ­sticas del proyecto

#### CoverageData
- MÃ©tricas de cobertura de cÃ³digo
- Desglose por archivo y mÃ³dulo
- Tendencias histÃ³ricas

### 2. Vistas del Dashboard

#### Dashboard Principal
- Resumen general del estado de las pruebas
- GrÃ¡ficos de tendencias
- Indicadores clave de rendimiento

#### Vistas por MÃ³dulo
- Desglose detallado por aplicaciÃ³n
- Historial de ejecuciones
- Cobertura de cÃ³digo especÃ­fica

#### DocumentaciÃ³n
- GuÃ­as de implementaciÃ³n de pruebas
- Ejemplos de cÃ³digo
- Buenas prÃ¡cticas

## Flujo de Datos

1. **EjecuciÃ³n de Pruebas**:
   - Las pruebas se ejecutan mediante comandos de gestiÃ³n o CI/CD
   - Los resultados se guardan en formato estÃ¡ndar (JUnit, Coverage.py)

2. **Procesamiento**:
   - Los comandos de gestiÃ³n procesan los resultados
   - Se actualizan los modelos de datos
   - Se generan mÃ©tricas agregadas

3. **VisualizaciÃ³n**:
   - Las vistas consultan los modelos de datos
   - Se presentan los datos de forma clara y accesible
   - Los usuarios pueden navegar entre diferentes niveles de detalle

## IntegraciÃ³n con el Proyecto

### ConfiguraciÃ³n Requerida

1. **settings.py**:
   ```python
   INSTALLED_APPS += ['core_testing']
   
   # ConfiguraciÃ³n de rutas para informes de pruebas
   TEST_RUNNER = 'core_testing.runner.PytestTestRunner'
   TEST_OUTPUT_DIR = BASE_DIR / 'test_results'
   ```

2. **CI/CD**:
   - Configurar para ejecutar pruebas y actualizar el dashboard
   - Ejemplo para GitHub Actions:
     ```yaml
     - name: Run Tests
       run: |
         python manage.py test
         python manage.py update_test_results
     ```

## PrÃ³ximos Pasos

1. Implementar los modelos de datos actualizados
2. Desarrollar las vistas del dashboard
3. Crear comandos de gestiÃ³n para actualizaciÃ³n automÃ¡tica
4. Documentar el proceso de integraciÃ³n

2. **RefactorizaciÃ³n (si es necesario)**:
   - Extraer lÃ³gica de negocio a funciones/mÃ©todos mÃ¡s pequeÃ±os
   - Aplicar principios SOLID
   - Asegurar bajo acoplamiento y alta cohesiÃ³n

3. **ImplementaciÃ³n de Pruebas**:
   - Crear pruebas que verifiquen el comportamiento actual
   - Asegurar que las pruebas fallen si el comportamiento cambia

## IntegraciÃ³n con Aplicaciones Existentes

### Para Aplicaciones Nuevas

1. Crear directorio `tests/` en la raÃ­z de la aplicaciÃ³n
2. Importar las utilidades de `core_testing`
3. Seguir la estructura de pruebas establecida

### Para Aplicaciones Existentes

1. Evaluar la necesidad de refactorizaciÃ³n
2. Crear pruebas para la funcionalidad crÃ­tica primero
3. Refactorizar gradualmente siguiendo las pruebas

## Buenas PrÃ¡cticas

1. **Nombrado de Pruebas**:
   - Usar nombres descriptivos que expliquen el comportamiento probado
   - Seguir el patrÃ³n: `test_<mÃ©todo>_<condiciÃ³n>_<resultado_esperado>`

2. **OrganizaciÃ³n**:
   - Agrupar pruebas relacionadas en clases
   - Usar fixtures de pytest para datos de prueba comunes
   - Mantener las pruebas independientes entre sÃ­

## ğŸ”§ Mantenimiento y Mejoras

### Mantenimiento Regular

1. **ActualizaciÃ³n de Pruebas**
   - Revisar y actualizar pruebas al modificar funcionalidades existentes
   - Eliminar pruebas obsoletas o redundantes
   - Mantener la cobertura de cÃ³digo por encima del 80%

2. **Rendimiento**
   - Monitorear el tiempo de ejecuciÃ³n de las pruebas
   - Optimizar pruebas lentas
   - Considerar el uso de fixtures para datos de prueba

3. **DocumentaciÃ³n**
   - Mantener actualizada la documentaciÃ³n del sistema
   - Documentar nuevas caracterÃ­sticas y cambios
   - Incluir ejemplos de uso

### Mejoras Futuras

1. **IntegraciÃ³n con CI/CD**
   - ConfiguraciÃ³n para GitHub Actions/GitLab CI
   - Notificaciones en tiempo real
   - Reportes automatizados

2. **AnÃ¡lisis Avanzado**
   - DetecciÃ³n de pruebas frÃ¡giles
   - AnÃ¡lisis de tendencias
   - Alertas automÃ¡ticas

3. **Mejoras en la Interfaz**
   - Filtros avanzados
   - ExportaciÃ³n de reportes
   - Vistas personalizables

## ğŸ“Š MÃ©tricas Clave de Rendimiento (KPI)

1. **Cobertura de CÃ³digo**
   - Objetivo: >80% de cobertura
   - Actual: [Por determinar]%

2. **Tiempo de EjecuciÃ³n**
   - Tiempo promedio: [Por determinar] segundos
   - Objetivo: < 5 minutos para todo el conjunto

3. **Estabilidad**
   - Tasa de Ã©xito: >95%
   - Pruebas inestables: <5%

## Ejemplo de Uso

### 1. Definir una Interfaz de Prueba

```python
# core_testing/testing_interfaces/ventas.py
from core_testing.testing_interfaces.base import TestingInterface

class VentaTestingInterface(TestingInterface):
    name = "ventas"
    description = "Pruebas para el mÃ³dulo de ventas"
    
    def test_crear_venta(self, cliente, productos):
        """Prueba la creaciÃ³n de una nueva venta"""
        # ImplementaciÃ³n de la prueba
        pass
```

### 2. Usar en una AplicaciÃ³n

```python
# ventas/tests/test_models.py
import pytest
from core_testing.testing_interfaces.ventas import VentaTestingInterface

class TestVentaModel(VentaTestingInterface):
    
    def test_crear_venta(self):
        # ConfiguraciÃ³n
        cliente = ClienteFactory()
        productos = [ProductoFactory() for _ in range(3)]
        
        # Ejecutar prueba de la interfaz
        resultado = super().test_crear_venta(cliente, productos)
        
        # Aserciones especÃ­ficas
        assert resultado.estado == 'completada'
        assert resultado.total > 0
```

## Consideraciones de Rendimiento

- Ejecutar pruebas en paralelo cuando sea posible
- Usar bases de datos en memoria para pruebas unitarias
- Limpiar datos de prueba despuÃ©s de cada ejecuciÃ³n
- Monitorear el tiempo de ejecuciÃ³n de las pruebas

## Seguridad

- No incluir datos sensibles en las pruebas
- Validar entradas incluso en pruebas
- Probar casos de error y condiciones de borde
- Verificar permisos y autenticaciÃ³n

## Mantenimiento

- Revisar periÃ³dicamente las dependencias de testing
- Actualizar las pruebas al actualizar versiones de Django
- Documentar cambios en las interfaces de prueba
